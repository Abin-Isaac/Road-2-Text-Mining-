# Road-2-Text-Mining-
## Library used - Natural Language Tool Kit

## Data Set - Wiki Corpus
The wiki corpus contains the full text of Wikipedia, and it contains 1.9 billion words in more than 4.4 million articles. We have used a subset of that "Wiki_corpus"


## Objective:

• Implement different types of tokenizers

• Perform stemming and lemmatization

• Evaluate POS tags for the tokens

• Remove the stop words and print NER words
